{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<h1><center>Predicting Financial Markets with Machine Learning      </center></h1>\n",
    "<h1><center>-      </center></h1>\n",
    "<h2><center>Non-Linear Models      </center></h2>\n",
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Purpose</h2>\n",
    "<br>\n",
    "<hr>\n",
    "A notebook to develop an AI system aiming at trading intraday on cryptocurrencies\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Imports</h2>\n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and Python\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Graphic Libraries\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"simple_white\"\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import matplotlib as plt\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "\n",
    "# AI and stats\n",
    "import statsmodels.api as sm\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Notebook Parameters</h2>\n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define data path\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Risk free rate assumption\n",
    "risk_free_rate = 0.05  # % per year\n",
    "rfr_hourly = (1 + risk_free_rate) ** (1 / (24 * 365)) - 1\n",
    "\n",
    "# Suggested training set\n",
    "start_date_train = \"2023-01-24\"\n",
    "last_date_train = \"2024-01-24\"\n",
    "\n",
    "# Suggested validation set\n",
    "start_date_validate = \"2024-01-25\"\n",
    "last_date_validate = \"2024-07-24\"\n",
    "\n",
    "# Test set (Unavailable)\n",
    "# start_date_test = \"2024-07-25\"\n",
    "# last_date_test = \"2025-01-24\"\n",
    "\n",
    "# Maximum number of features to use\n",
    "max_nb_features = 20\n",
    "\n",
    "# Set a level of transaction costs\n",
    "tc = 0.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Data Loading</h2>\n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data\n",
    "data = pd.read_csv(\n",
    "    f\"{data_path}data_in_sample.csv\",\n",
    "    index_col=0,\n",
    "    header=[0, 1],\n",
    ")\n",
    "\n",
    "# Make sure that the index is in the right format\n",
    "data.index = pd.to_datetime(data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature_914682300606.csv\n",
      "Loading feature_929491324974.csv\n",
      "Loading feature_936020771498.csv\n",
      "Loading feature_936377278430.csv\n",
      "Loading feature_938545158299.csv\n",
      "Loading feature_941272397662.csv\n",
      "Loading feature_950281828683.csv\n",
      "Loading feature_950691475264.csv\n",
      "Loading feature_952144189258.csv\n",
      "Loading feature_955845031686.csv\n",
      "Loading feature_961547632365.csv\n",
      "Loading feature_962177675988.csv\n",
      "Loading feature_962320961442.csv\n",
      "Loading feature_965788928669.csv\n",
      "Loading feature_977512281584.csv\n",
      "Loading feature_983617492082.csv\n",
      "Loading feature_986307325973.csv\n",
      "Loading feature_993937281321.csv\n",
      "Loading feature_996678870868.csv\n",
      "Loading feature_997041357627.csv\n"
     ]
    }
   ],
   "source": [
    "# Load pre-processed features\n",
    "features = {}\n",
    "for dirpath, dirnames, filenames in os.walk(data_path):\n",
    "    for filename in filenames[-max_nb_features:]:\n",
    "        if \"feature\" in filename:\n",
    "            print(f\"Loading {filename}\")\n",
    "\n",
    "            # Load feature\n",
    "            feature = pd.read_csv(\n",
    "                f\"{data_path}{filename}\",\n",
    "                index_col=0,\n",
    "                header=[0],\n",
    "            )\n",
    "\n",
    "            # Make sure that the index is in the right format\n",
    "            feature.index = pd.to_datetime(feature.index)\n",
    "\n",
    "            # Store in the feature dict\n",
    "            features[filename.replace(\".csv\", \"\")] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_914682300606',\n",
       " 'feature_929491324974',\n",
       " 'feature_936020771498',\n",
       " 'feature_936377278430',\n",
       " 'feature_938545158299',\n",
       " 'feature_941272397662',\n",
       " 'feature_950281828683',\n",
       " 'feature_950691475264',\n",
       " 'feature_952144189258',\n",
       " 'feature_955845031686',\n",
       " 'feature_961547632365',\n",
       " 'feature_962177675988',\n",
       " 'feature_962320961442',\n",
       " 'feature_965788928669',\n",
       " 'feature_977512281584',\n",
       " 'feature_983617492082',\n",
       " 'feature_986307325973',\n",
       " 'feature_993937281321',\n",
       " 'feature_996678870868',\n",
       " 'feature_997041357627']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Analytics</h2>\n",
    "<br>\n",
    "<hr>\n",
    "Basic Portfolio analytics to invest in some predictions of the future instruments returns\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_returns_to_positions(expected_returns):\n",
    "    \"\"\"\n",
    "    Normalize expected returns to make it an investable portfolio\n",
    "\n",
    "    :param expected_returns: pd.DataFrame containing expectations\n",
    "                             about future instruments prices variations\n",
    "    \"\"\"\n",
    "\n",
    "    # Positions will be proportional to ranked alpha\n",
    "    positions = expected_returns.rank(axis=1)\n",
    "\n",
    "    # Re-scale the leverage\n",
    "    positions = positions.div(positions.abs().sum(axis=1), axis=0)\n",
    "\n",
    "    # Make the portfolio dollar neutral\n",
    "    positions = positions.sub(positions.mean(axis=1), axis=0)\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def get_sharpe(pnl_portfolio, rfr_hourly):\n",
    "    \"\"\"\n",
    "    Compute the sharpe ratio\n",
    "\n",
    "    :param pnl_portfolio: pd.Series of returns of the portfolio considered\n",
    "    :param rfr_hourly: float, the hourly risk free rate\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute excess returns\n",
    "    excess_returns = pnl_portfolio - rfr_hourly\n",
    "\n",
    "    # Compute sharpe ratio\n",
    "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(24 * 365)\n",
    "\n",
    "    # Output\n",
    "    return round(sharpe_ratio, 2)\n",
    "\n",
    "\n",
    "def pnl_analytics(positions, returns, rfr_hourly, lag, tc=0):\n",
    "    \"\"\"\n",
    "    Compute the p&l analytics of the strategy\n",
    "\n",
    "    :param positions: pd.DataFrame, some positions that have been reached\n",
    "    :param returns: pd.DataFrame containing returns of instruments\n",
    "    :param rfr_hourly: float, the hourly risk free rate\n",
    "    :param lag: int, the number of hours to reach the positions\n",
    "    :param tc: float, the transaction costs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute gross p&l\n",
    "    pnl = positions.shift(1 + lag).mul(returns).sum(axis=1)\n",
    "\n",
    "    # Compute transaction costs\n",
    "    trades = positions.fillna(0).diff()\n",
    "    costs = trades.abs().sum(axis=1) * tc\n",
    "\n",
    "    # Net p&l: deduce costs from gross p&l\n",
    "    pnl = pnl.sub(costs, fill_value=0)\n",
    "\n",
    "    # Compute sharpe\n",
    "    sharpe = get_sharpe(pnl, rfr_hourly)\n",
    "\n",
    "    return {\"sharpe\": sharpe, \"pnl\": pnl}\n",
    "\n",
    "\n",
    "def analyze_expected_returns(\n",
    "    expected_returns,\n",
    "    returns,\n",
    "    rfr_hourly,\n",
    "    title=\"a Nice Try\",\n",
    "    lags=[0, 1, 2, 3, 6, 12],\n",
    "    tc=0,\n",
    "    output_sharpe=False,\n",
    "    display_results=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Provide an economic analysis of some expected_returns\n",
    "\n",
    "    :param expected_returns: pd.DataFrame containing expectations\n",
    "                             about future instruments prices variations\n",
    "    \"\"\"\n",
    "\n",
    "    # Take positions as a function of expected returns\n",
    "    positions = expected_returns_to_positions(expected_returns)\n",
    "\n",
    "    # Compute p&l and sharpe for different lags\n",
    "    pnl_lags = {}\n",
    "    for lag in lags:\n",
    "        analytics_lag = pnl_analytics(\n",
    "            positions=positions, returns=returns, rfr_hourly=rfr_hourly, lag=lag, tc=tc\n",
    "        )\n",
    "        lag_label = f\"Lag {lag}, sharpe={analytics_lag['sharpe']}\"\n",
    "        pnl_lags[lag_label] = analytics_lag[\"pnl\"]\n",
    "\n",
    "    # Display returns\n",
    "    pnl_lags = pd.concat(pnl_lags, axis=1).dropna()\n",
    "    if display_results:\n",
    "        fig = (\n",
    "            (1 + pnl_lags)\n",
    "            .cumprod()\n",
    "            .plot(\n",
    "                title=f\"Cumulative returns of {title}\",\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "        fig.show()\n",
    "\n",
    "    if output_sharpe:\n",
    "        for lag_label in pnl_lags.columns:\n",
    "            if \"Lag 0\" in lag_label:\n",
    "                return lag_label.split(\"sharpe=\")[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Features Standard Pre-Processing</h2>\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[\"return\"].loc[start_date_train:last_date_train].shift(-1).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature_914682300606\n",
      "Processing feature_929491324974\n",
      "Processing feature_936020771498\n",
      "Processing feature_936377278430\n",
      "Processing feature_938545158299\n",
      "Processing feature_941272397662\n",
      "Processing feature_950281828683\n",
      "Processing feature_950691475264\n",
      "Processing feature_952144189258\n",
      "Processing feature_955845031686\n",
      "Processing feature_961547632365\n",
      "Processing feature_962177675988\n",
      "Processing feature_962320961442\n",
      "Processing feature_965788928669\n",
      "Processing feature_977512281584\n",
      "Processing feature_983617492082\n",
      "Processing feature_986307325973\n",
      "Processing feature_993937281321\n",
      "Processing feature_996678870868\n",
      "Processing feature_997041357627\n"
     ]
    }
   ],
   "source": [
    "features_normalized = {}\n",
    "\n",
    "for feature_name in features.keys():\n",
    "    print(f\"Processing {feature_name}\")\n",
    "\n",
    "    # Extract the feature\n",
    "    feature_normalized = features[feature_name]\n",
    "\n",
    "    # Rank the feature to remove outliers\n",
    "    feature_normalized = feature_normalized.rank(axis=1, pct=True) - 0.5\n",
    "\n",
    "    # Stack the feature\n",
    "    feature_normalized = feature_normalized.stack().sort_index()\n",
    "\n",
    "    # Store this normalized version\n",
    "    features_normalized[feature_name] = feature_normalized\n",
    "\n",
    "# Convert normalized features dict to a single dataframe\n",
    "features_normalized = pd.concat(features_normalized, axis=1)\n",
    "\n",
    "# Replace NaNs by average values, as OLS cannot handle NaNs effectively\n",
    "features_normalized = features_normalized.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Non-Linear Polymodels</h2>\n",
    "<br>\n",
    "<hr>\n",
    "By introducing non-linearities in our polymodel, we aim at capturing better the complexity of the data\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4>Re-process features to get standardized polynomials</h4>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature_914682300606\n",
      "Processing feature_929491324974\n",
      "Processing feature_936020771498\n",
      "Processing feature_936377278430\n",
      "Processing feature_938545158299\n",
      "Processing feature_941272397662\n",
      "Processing feature_950281828683\n",
      "Processing feature_950691475264\n",
      "Processing feature_952144189258\n",
      "Processing feature_955845031686\n",
      "Processing feature_961547632365\n",
      "Processing feature_962177675988\n",
      "Processing feature_962320961442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     26\u001b[0m feature_normalized \u001b[38;5;241m=\u001b[39m feature_normalized\u001b[38;5;241m.\u001b[39mdiv(\n\u001b[0;32m     27\u001b[0m     feature_normalized\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Stack the feature\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m feature_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Avoid NaNs values\u001b[39;00m\n\u001b[0;32m     34\u001b[0m feature_normalized \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     35\u001b[0m     feature_normalized\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreindex(full_stacked_index)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ameya\\miniconda3\\envs\\finance\\lib\\site-packages\\pandas\\core\\series.py:4069\u001b[0m, in \u001b[0;36mSeries.sort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   3936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_index\u001b[39m(\n\u001b[0;32m   3937\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3938\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3947\u001b[0m     key: IndexKeyFunc \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3948\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3949\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3950\u001b[0m \u001b[38;5;124;03m    Sort Series by index labels.\u001b[39;00m\n\u001b[0;32m   3951\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4066\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   4067\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4073\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4076\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_remaining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_remaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ameya\\miniconda3\\envs\\finance\\lib\\site-packages\\pandas\\core\\generic.py:5309\u001b[0m, in \u001b[0;36mNDFrame.sort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   5305\u001b[0m ascending \u001b[38;5;241m=\u001b[39m validate_ascending(ascending)\n\u001b[0;32m   5307\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 5309\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mget_indexer_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_remaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[0;32m   5311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32mc:\\Users\\ameya\\miniconda3\\envs\\finance\\lib\\site-packages\\pandas\\core\\sorting.py:92\u001b[0m, in \u001b[0;36mget_indexer_indexer\u001b[1;34m(target, level, ascending, kind, na_position, sort_remaining, key)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any], Index, Series]\", variable has\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# type \"Index\")\u001b[39;00m\n\u001b[0;32m     91\u001b[0m target \u001b[38;5;241m=\u001b[39m ensure_key_mapped(target, key, levels\u001b[38;5;241m=\u001b[39mlevel)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort_levels_monotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     _, indexer \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msortlevel(\n\u001b[0;32m     96\u001b[0m         level,\n\u001b[0;32m     97\u001b[0m         ascending\u001b[38;5;241m=\u001b[39mascending,\n\u001b[0;32m     98\u001b[0m         sort_remaining\u001b[38;5;241m=\u001b[39msort_remaining,\n\u001b[0;32m     99\u001b[0m         na_position\u001b[38;5;241m=\u001b[39mna_position,\n\u001b[0;32m    100\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ameya\\miniconda3\\envs\\finance\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:2050\u001b[0m, in \u001b[0;36mMultiIndex._sort_levels_monotonic\u001b[1;34m(self, raise_if_incomparable)\u001b[0m\n\u001b[0;32m   2016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sort_levels_monotonic\u001b[39m(\u001b[38;5;28mself\u001b[39m, raise_if_incomparable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultiIndex:\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;124;03m    This is an *internal* function.\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2048\u001b[0m \u001b[38;5;124;03m               )\u001b[39;00m\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2050\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_lexsorted() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_monotonic_increasing\u001b[49m:\n\u001b[0;32m   2051\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   2053\u001b[0m     new_levels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ameya\\miniconda3\\envs\\finance\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:1710\u001b[0m, in \u001b[0;36mMultiIndex.is_monotonic_increasing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"lexsort\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m     \u001b[38;5;66;03m# \"List[Union[ExtensionArray, ndarray[Any, Any]]]\";\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;66;03m# int, float, complex, str, bytes, _NestedSequence[Union\u001b[39;00m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;66;03m# [bool, int, float, complex, str, bytes]]]\"\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m     sort_order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlexsort(values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43msort_order\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mis_monotonic_increasing\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1712\u001b[0m     \u001b[38;5;66;03m# we have mixed types and np.lexsort is not happy\u001b[39;00m\n\u001b[0;32m   1713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\u001b[38;5;241m.\u001b[39mis_monotonic_increasing\n",
      "File \u001b[1;32mc:\\Users\\ameya\\miniconda3\\envs\\finance\\lib\\site-packages\\pandas\\core\\indexes\\base.py:475\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    470\u001b[0m _references \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# Constructors\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__new__\u001b[39m(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    477\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    478\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    479\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    480\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    481\u001b[0m     tupleize_cols: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    482\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrange\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RangeIndex\n\u001b[0;32m    485\u001b[0m     name \u001b[38;5;241m=\u001b[39m maybe_extract_name(name, data, \u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "polynomials = {}\n",
    "\n",
    "# Create a complete stacked index so that all the features afterwards\n",
    "# share the same index\n",
    "full_stacked_index = data[\"return\"].fillna(0).stack().sort_index().index\n",
    "\n",
    "for feature_name in features.keys():\n",
    "    print(f\"Processing {feature_name}\")\n",
    "\n",
    "    # Extract the feature\n",
    "    feature_base = features[feature_name]\n",
    "\n",
    "    # Make sure that 'event' features are only treated when active\n",
    "    feature_base = feature_base.replace(0, np.nan).dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Rank the feature to remove outliers\n",
    "    feature_base = feature_base.rank(axis=1, pct=True) - 0.5\n",
    "\n",
    "    polynomials_feature = {}\n",
    "\n",
    "    for degree in [1, 2, 3, 4]:\n",
    "        # Elevate the feature too a given degree\n",
    "        feature_normalized = feature_base**degree\n",
    "\n",
    "        # Normalize to avoid the optimization to collapse in sklearn\n",
    "        feature_normalized = feature_normalized.div(\n",
    "            feature_normalized.std(axis=1), axis=0\n",
    "        )\n",
    "\n",
    "        # Stack the feature\n",
    "        feature_normalized = feature_normalized.stack().sort_index()\n",
    "\n",
    "        # Avoid NaNs values\n",
    "        feature_normalized = (\n",
    "            feature_normalized.fillna(0).reindex(full_stacked_index).fillna(0)\n",
    "        )\n",
    "\n",
    "        # Create hermite polynomials\n",
    "        polynomials_feature[f\"X{degree}\"] = feature_normalized\n",
    "\n",
    "    # Store all the polynomials together\n",
    "    polynomials[feature_name] = pd.concat(polynomials_feature, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4>Training polymodels</h4>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Measure time\n",
    "t1 = time.time()\n",
    "\n",
    "# Recompute the model every month, skip the first 2 months\n",
    "rebalancing_dates = pd.date_range(\n",
    "    start=start_date_train, end=last_date_validate, freq=\"ME\"\n",
    ")[2:]\n",
    "\n",
    "\n",
    "def train_predict_period(\n",
    "    last_date_train_fold,\n",
    "    returns,\n",
    "):\n",
    "    # Define training and validation dates\n",
    "\n",
    "    # Train the model over the last X months\n",
    "    start_date_train_fold = last_date_train_fold - pd.Timedelta(days=30 * 1)\n",
    "\n",
    "    # The model cannot be used before the first day following the training\n",
    "    # (no look-forward bias)\n",
    "    start_date_validate_fold = last_date_train_fold + pd.Timedelta(days=1)\n",
    "\n",
    "    # The trained model will be used for 1 month\n",
    "    last_date_validate_fold = last_date_train_fold + pd.Timedelta(days=31 * 1)\n",
    "\n",
    "    # Log informations\n",
    "    print(\n",
    "        f\"Train a model from date {start_date_train_fold} to date {last_date_train_fold}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Predict from date {start_date_validate_fold} to date {last_date_validate_fold}\"\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "    # Create label\n",
    "    label_fold = (\n",
    "        returns.loc[start_date_train_fold:last_date_train_fold].shift(-1).stack()\n",
    "    )\n",
    "\n",
    "    # Initialize predictions final dataframe\n",
    "    predictions_date = pd.DataFrame(\n",
    "        0,\n",
    "        index=returns.index,\n",
    "        columns=returns.columns,\n",
    "    ).loc[start_date_validate_fold:last_date_validate_fold]\n",
    "\n",
    "    for feature_name in polynomials.keys():\n",
    "        print(feature_name)\n",
    "\n",
    "        # Extract polynomials for the feature\n",
    "        polynomials_feature = polynomials[feature_name]\n",
    "\n",
    "        # Make sure all training data is aligned\n",
    "        polynomials_feature_train_fold = polynomials_feature.loc[\n",
    "            start_date_train_fold:last_date_train_fold\n",
    "        ]\n",
    "        common_index = label_fold.index.intersection(\n",
    "            polynomials_feature_train_fold.index\n",
    "        )\n",
    "        polynomials_feature_train_fold = polynomials_feature_train_fold.loc[\n",
    "            common_index\n",
    "        ]\n",
    "        label_fold_feature = label_fold.loc[common_index]\n",
    "\n",
    "        # Get polynomials for predictions\n",
    "        polynomials_feature_validate_fold = polynomials_feature.loc[\n",
    "            start_date_validate_fold:last_date_validate_fold\n",
    "        ]\n",
    "\n",
    "        # Create model\n",
    "        model = sklearn.linear_model.ElasticNetCV(\n",
    "            l1_ratio=0.01,\n",
    "            fit_intercept=True,\n",
    "            cv=5,\n",
    "            alphas=np.logspace(-8, -5, 50),\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        model = model.fit(\n",
    "            y=label_fold_feature,\n",
    "            X=polynomials_feature_train_fold,\n",
    "        )\n",
    "\n",
    "        # Predict on the validation set\n",
    "        predictions_feature = model.predict(polynomials_feature_validate_fold)\n",
    "        predictions_feature = pd.Series(\n",
    "            predictions_feature, index=polynomials_feature_validate_fold.index\n",
    "        ).unstack()\n",
    "\n",
    "        # Aggregate with other predictions\n",
    "        predictions_date = predictions_date.add(predictions_feature, fill_value=0)\n",
    "\n",
    "    print(f\"date {last_date_train_fold} done\")\n",
    "\n",
    "    # Output results\n",
    "    return pd.concat({str(last_date_train_fold): predictions_date}, axis=1)\n",
    "\n",
    "\n",
    "# Fix all but one function parameters to iterate on the last one\n",
    "partial_train_predict_period = partial(\n",
    "    train_predict_period,\n",
    "    returns=data[\"return\"],\n",
    ")\n",
    "\n",
    "# Train using one core per date\n",
    "with Pool(1) as pool:\n",
    "    predictions = pool.map(\n",
    "        partial_train_predict_period,  # function to multiprocess\n",
    "        rebalancing_dates,  # values to iterate on\n",
    "    )\n",
    "\n",
    "# Reformat predictions\n",
    "predictions = pd.concat(predictions, axis=1).T.groupby(level=1).sum().T\n",
    "\n",
    "\n",
    "# Training finished, print time used for it\n",
    "t2 = time.time()\n",
    "print(f\"Total Training time is {t2 - t1} seconds\")\n",
    "\n",
    "# Analyse our predictions\n",
    "analyze_expected_returns(\n",
    "    expected_returns=predictions.loc[start_date_validate:last_date_validate],\n",
    "    returns=data[\"return\"].loc[start_date_validate:last_date_validate],\n",
    "    rfr_hourly=rfr_hourly,\n",
    "    title=f\"Non-Linear Polymodel, Walk-Forward Cross-Validation, Validation Set\",\n",
    "    lags=[0, 1, 2, 3, 6, 12],\n",
    "    tc=tc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>XGBoost: Gradient Boosted Decision Trees</h2>\n",
    "<br>\n",
    "<hr>\n",
    "Gradient Boosted Decision Trees are another way to introduce non-linearity in our model. This non-linearity is present in the link between the label and features, but also among the features themselves. Overfitting is limited thanks to a variety of strategies, resulting in potentially better generalization.\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4>Defining hyper-parameters</h4>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"n_estimators\": 500,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"base_score\": 0,\n",
    "    \"max_depth\": 7,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"subsample\": 0.05,\n",
    "    \"colsample_bytree\": 0.3,\n",
    "    \"min_split_loss\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"n_jobs\": 1,\n",
    "    \"random_state\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4>Training the models</h4>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "t1 = time.time()\n",
    "\n",
    "# Recompute the model every month, skip the first 2 months\n",
    "rebalancing_dates = pd.date_range(\n",
    "    start=start_date_train, end=last_date_validate, freq=\"ME\"\n",
    ")[2:]\n",
    "\n",
    "\n",
    "def train_predict_period(\n",
    "    last_date_train_fold,\n",
    "    returns,\n",
    "    hyperparameters,\n",
    "):\n",
    "    # Define training and validation dates\n",
    "\n",
    "    # Train the model over the last X months\n",
    "    start_date_train_fold = last_date_train_fold - pd.Timedelta(days=30 * 12)\n",
    "\n",
    "    # The model cannot be used before the first day following the training\n",
    "    # (no look-forward bias)\n",
    "    start_date_validate_fold = last_date_train_fold + pd.Timedelta(days=1)\n",
    "\n",
    "    # The trained model will be used for 1 month\n",
    "    last_date_validate_fold = last_date_train_fold + pd.Timedelta(days=31 * 1)\n",
    "\n",
    "    # Log informations\n",
    "    print(\n",
    "        f\"Train a model from date {start_date_train_fold} to date {last_date_train_fold}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Predict from date {start_date_validate_fold} to date {last_date_validate_fold}\"\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "    # Create label\n",
    "    label_fold = (\n",
    "        returns.loc[start_date_train_fold:last_date_train_fold].shift(-1).stack()\n",
    "    )\n",
    "\n",
    "    # Only keep dates of the train and validation sets for the features\n",
    "    features_normalized_train_fold = features_normalized.reindex(label_fold.index)\n",
    "    features_normalized_validate_fold = features_normalized.sort_index().loc[\n",
    "        start_date_validate_fold:last_date_validate_fold\n",
    "    ]\n",
    "\n",
    "    # Split the data along the time axis\n",
    "    ts_splitter = sklearn.model_selection.TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Create model\n",
    "    model = XGBRegressor(**hyperparameters)\n",
    "\n",
    "    # Fit model\n",
    "    model = model.fit(\n",
    "        y=label_fold,\n",
    "        X=features_normalized_train_fold,\n",
    "    )\n",
    "\n",
    "    # Predict on the validation set\n",
    "    predictions = model.predict(features_normalized_validate_fold)\n",
    "    predictions = pd.Series(\n",
    "        predictions, index=features_normalized_validate_fold.index\n",
    "    ).unstack()\n",
    "\n",
    "    # Output results\n",
    "    return pd.concat({str(last_date_train_fold): predictions}, axis=1)\n",
    "\n",
    "\n",
    "# Fix all but one function parameters to iterate on the last one\n",
    "partial_train_predict_period = partial(\n",
    "    train_predict_period,\n",
    "    returns=data[\"return\"],\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# Train using one core per date\n",
    "with Pool(16) as pool:\n",
    "    predictions = pool.map(\n",
    "        partial_train_predict_period,  # function to multiprocess\n",
    "        rebalancing_dates,  # values to iterate on\n",
    "    )\n",
    "\n",
    "# Reformat predictions\n",
    "predictions = pd.concat(predictions, axis=1).T.groupby(level=1).sum().T\n",
    "\n",
    "\n",
    "# Training finished, print time used for it\n",
    "t2 = time.time()\n",
    "print(f\"Total Training time is {t2 - t1} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse our predictions\n",
    "analyze_expected_returns(\n",
    "    expected_returns=predictions.loc[start_date_validate:last_date_validate],\n",
    "    returns=data[\"return\"].loc[start_date_validate:last_date_validate],\n",
    "    rfr_hourly=rfr_hourly,\n",
    "    title=f\"Gradient Boosted Trees, Walk-Forward Cross-Validation, Validation Set\",\n",
    "    lags=[0, 1, 2, 3, 6, 12],\n",
    "    tc=tc,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
